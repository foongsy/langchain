{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Basic Usage (LLM + ChatModel only)\n",
    "Langchainçš„æ¡†æž¶ä¸‹é¢æœ‰ä¸»è¦åˆ†ç‚ºå…©ç¨®Model\n",
    "- LLM\n",
    "- Chat\n",
    "\n",
    "ç”±æ–¼Langchainä¸¦ä¸æä¾›ä»»ä½•Modelï¼Œæ‰€ä»¥LLMç›¸é—œçš„libraryéƒ½ä¸åœ¨langchain-coreè£¡é¢\n",
    "\n",
    "#### Langchainæœ€åŸºæœ¬çš„ç”¨æ³•\n",
    "`Prompt -> LLM -> Response`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything starts with LLM\n",
    "from langchain_core.messages import HumanMessage\n",
    "msg = HumanMessage(content='èŒ¶é¤å»³æ˜¯ä»€éº¼ï¼Ÿ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ä½¿ç”¨ChatModel (e.g. TogetherAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "model = ChatTogether(\n",
    "    together_api_key=os.environ['KEY_TOGETHERAI'],\n",
    "    model=\"Qwen/Qwen1.5-72B-Chat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke([msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### åˆ©ç”¨output_parserå°‡outputè®Šå¾—æ›´æ˜“è®€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "p = StrOutputParser()\n",
    "# print(type(p))\n",
    "print(p.invoke(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### åŠ å…¥SystemMessage (Instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "system_message = \"\"\"\n",
    "å¿…é ˆä»¥ç¹é«”ä¸­æ–‡å›žç­”\n",
    "å›žç­”æ™‚åªä»¥','åˆ†éš”æ¯å€‹é …ç›®ï¼Œä¸¦ä»¥æœ€ç°¡çŸ­çš„æ–¹å¼å›žç­”ï¼Œä¾‹å¦‚ï¼š\n",
    "ä¸€,äºŒ,ä¸‰,...\n",
    "\"\"\"\n",
    "human_message = \"\"\"\n",
    "è«‹åˆ—èˆ‰èŒ¶é¤å»³ä¸å¤šéŽ10ç¨®æœ€ç†±é–€çš„é£Ÿç‰©ï¼Œä¸è‚¯å®šçš„ä¸è¦å›žç­”\n",
    "\"\"\"\n",
    "msg_list = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=human_message)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(msg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.invoke(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Structured Data Parser\n",
    "https://python.langchain.com/v0.2/docs/concepts/#output-parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csvp = CommaSeparatedListOutputParser()\n",
    "parsed_res = csvp.invoke(res)\n",
    "print(type(parsed_res))\n",
    "print(parsed_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ç”¨PromptTemplateæ›´æ–¹ä¾¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", \"è«‹åˆ—èˆ‰{place}ä¸å¤šéŽ10ç¨®æœ€ç†±é–€çš„é£Ÿç‰©ï¼Œä¸è‚¯å®šçš„ä¸è¦å›žç­”\"),\n",
    "])\n",
    "\n",
    "prompt_value = prompt_template.invoke(\n",
    "    {\n",
    "        'place': 'æ—¥æœ¬'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "japan_csv = csvp.invoke(model.invoke(prompt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(japan_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ä»¥steamingçš„æ–¹å¼å›žç­”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_system_message = \"\"\"ä½ æ˜¯ä¸€å€‹æ—¥æœ¬è¼•å°èªªä½œå®¶ï¼Œå¯«ä½œé¢¨æ ¼è¦æ¨¡ä»¿{author}ï¼Œè«‹ä»¥ç”¨æˆ¶çš„åå­—å‘½åä¸»è§’ï¼Œä¸¦ä»¥æä¾›çš„é—œéµå­—åŽ»å¯«ä¸€å€‹å¤§ç´„400å­—çš„çŸ­ç¯‡æ•…äº‹ã€‚æ•…äº‹å¿…é ˆä»¥ç¹é«”ä¸­æ–‡ç·¨å¯«ã€‚\"\"\"\n",
    "prompt_template2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", author_system_message),\n",
    "    (\"ai\", \"è«‹å•ä½ æ€Žæ¨£ç¨±å‘¼ï¼Ÿ\"),\n",
    "    (\"human\", \"æˆ‘çš„åå­—å«{username}\"),\n",
    "    (\"ai\", \"ä½ å¥½{username}ï¼Œè«‹å•ä½ æƒ³ä»¥ä»€éº¼ä½œé¡Œæå¯«é€™å€‹æ•…äº‹å‘¢ï¼Ÿ\"),\n",
    "    (\"human\", \"æˆ‘æƒ³ä»¥{topics}ç‚ºé¡Œæå¯«é€™å€‹æ•…äº‹\"),\n",
    "])\n",
    "prompt_value = prompt_template2.invoke(\n",
    "    {\n",
    "        \"author\": \"å·åŽŸç¤«\",\n",
    "        \"username\": \"å½Œè±†å­\",\n",
    "        \"topics\": \"é¬¼ï¼Œæ‰é¬¼ï¼Œè¦ªæƒ…ï¼Œæ‹¯æ•‘ä¸–ç•Œ\"\n",
    "    }\n",
    ")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in model.stream(prompt_value):\n",
    "    print(res.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LCEL: LangChain Expression Language\n",
    "æ€Žæ¨£ç”¨LCELç°¡åŒ–æ—¥æœ¬è¼•å°èªªå®¶çš„Chainå‘¢ï¼Ÿ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnovel = prompt_template2 | model | p\n",
    "res = jnovel.invoke(\n",
    "    {\n",
    "        \"author\": \"æ”¯å€‰å‡ç ‚\",\n",
    "        \"username\": \"äº¨åˆ©\",\n",
    "        \"topics\": \"ç‹¼ï¼Œé¦™è¾›æ–™ï¼ŒåŒ—æ–¹ï¼Œæ—…è¡Œå•†äºº\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.parse(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnovel2 = prompt_template2 | model | p\n",
    "for res in jnovel2.stream(\n",
    "    {\n",
    "        \"author\": \"æ”¯å€‰å‡ç ‚\",\n",
    "        \"username\": \"äº¨åˆ©\",\n",
    "        \"topics\": \"ç‹¼ï¼Œé¦™è¾›æ–™ï¼ŒåŒ—æ–¹ï¼Œæ—…è¡Œå•†äºº\"\n",
    "    }):\n",
    "    print(res, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### å•é¡Œï¼šPrintçš„ç‚ºä½•ä¸å†æ˜¯res.content?\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/how_to/streaming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‰å¤§çš„LangChain: Runnable\n",
    "https://python.langchain.com/v0.2/docs/concepts/#runnable-interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Track: Tools Calling ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Note that the docstrings here are crucial, as they will be passed along\n",
    "# to the model along with the class name.\n",
    "class Add(BaseModel):\n",
    "    \"\"\"Add two integers together.\"\"\"\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "class Multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistralai/Mixtral-8x22B-Instruct-v0.1\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "better_model = ChatOpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=os.environ[\"KEY_TOGETHERAI\"],\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\n",
    "\n",
    "model_with_tools = better_model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': 'call_c9h4fp3hkehqluo2e4c7hh89'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'call_jbh3b9zk93vca027p231cjgk'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "model_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_h91xqdgcbuvujsfw14qr1tam', 'function': {'arguments': '{\"a\":3,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_crxrdmkmz3azw0jmj7x79fkt', 'function': {'arguments': '{\"a\":11,\"b\":49}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 304, 'total_tokens': 388}, 'model_name': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-59feef45-3db1-4be2-846d-48066dad7e5f-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_h91xqdgcbuvujsfw14qr1tam'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_crxrdmkmz3azw0jmj7x79fkt'}], usage_metadata={'input_tokens': 304, 'output_tokens': 84, 'total_tokens': 388}),\n",
       " ToolMessage(content='36', tool_call_id='call_h91xqdgcbuvujsfw14qr1tam'),\n",
       " ToolMessage(content='60', tool_call_id='call_crxrdmkmz3azw0jmj7x79fkt')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 438, 'total_tokens': 439}, 'model_name': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'system_fingerprint': None, 'finish_reason': 'eos', 'logprobs': None}, id='run-f9b725bb-bcde-450b-a888-9a32edff03f1-0', usage_metadata={'input_tokens': 438, 'output_tokens': 1, 'total_tokens': 439})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
